# -*- coding: utf-8 -*-
"""Delhivery_Case_Study.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrYUSO5noIcfLBa-_tsBDzfUT_hbf5ns

# **Delhivery Business Case Study**

**-By S D Preetham**

### **Problem Statement:**

 **Optimizing Logistics Efficiency: Feature Engineering & Data Analysis for Delhivery**
* Open Source Routing Machine (OSRM) is a high-performance routing engine that calculates the fastest or shortest path between locations using OpenStreetMap (OSM) data. It is widely used for logistics, navigation, and delivery optimization.

* **Delhivery**, a logistics company, relies on OSRM-based time and distance predictions for optimizing parcel deliveries. However, discrepancies between OSRM estimates and actual trip data here lead to inefficiencies, delays and inaccurate routing decisions. The primary objective here, is to analyze and improve OSRM's accuracy by comparing it with real-world data.

### **Approach**

* The approach involves cleaning and preprocessing the data, handling missing values and extracting structured features like city, state and timestamps.
* We then engineer new features such as the trip duration and compare OSRM-predicted vs actual time and distance to identify discrepancies. Using statistical analysis and hypothesis testing, we evaluate OSRM’s accuracy, detect overestimations or underestimations and assess their impact.
* Outliers are identified and handled using visualization and the IQR method. Finally, feature scaling (MinMaxScaler/StandardScaler) and one-hot encoding are applied for better model performance. Insights derived help optimize OSRM’s predictions, refine routing decisions and enhance Delhivery’s logistics efficiency.
"""

import pandas as pd
df = pd.read_csv('delhivery.csv')
df.head()

"""### **Basic data cleaning and exploration:**

*   Analyzing the structure of the data



"""

df.shape

"""It is observed that the dataset consists of 144867 rows and 24 columns."""

df.info()

"""

*   **Handling missing values in the data**"""

missing_values = df.isnull().sum()
missing_values

"""Insight: It is evident that there a few missing values in source name and destination name columns."""

# Imputing the null values
source_mapping = df.dropna(subset=['source_name']).groupby('source_center')['source_name'].first().to_dict()
destination_mapping = df.dropna(subset=['destination_name']).groupby('destination_center')['destination_name'].first().to_dict()



df['source_name'] = df.apply(lambda row: source_mapping.get(row['source_center'], row['source_name']), axis=1)
df['destination_name'] = df.apply(lambda row: destination_mapping.get(row['destination_center'], row['destination_name']), axis=1)

null_source = df['source_name'].isnull().sum()
null_destination = df['destination_name'].isnull().sum()

null_source, null_destination

"""**Approach to impute null values:**
* It has been previously observed that there are missing values in source_name and destination_name columns and these values are to be imputed.
* We also observed that there are no missing values in source_center and destination_center. Hence, we make a dictionary to map the source_center/destination_center to source_name/destination_name and use the .apply() to assign the source_name/ destination_name from the corresponding source_center/destination_center in the dictionary.
"""

agg_funcs = {
    'route_schedule_uuid': 'first',
    'route_type': 'first',
    'trip_creation_time': 'first',
    'source_name': 'first',   # First source (starting point)
    'destination_name': 'last',  # Last destination (final delivery point)
    'od_start_time': 'first',
    'od_end_time': 'last',
    'start_scan_to_end_scan': 'sum',
    'actual_distance_to_destination': 'sum',
    'actual_time': 'sum',
    'osrm_time': 'sum',
    'osrm_distance': 'sum',
    'segment_actual_time': 'sum',
    'segment_osrm_time': 'sum',
    'segment_osrm_distance': 'sum',
    'is_cutoff': 'first',
    'cutoff_factor': 'first',
    'cutoff_timestamp': 'first',
    'factor': 'first',
    'segment_factor': 'first'
}
# Step 1: Group by Trip UUID + Source + Destination
df_grouped = df.groupby(['trip_uuid', 'source_center', 'destination_center']).agg(agg_funcs).reset_index()
df_grouped

agg_funcs = {
    'route_schedule_uuid': 'first',
    'route_type': 'first',
    'trip_creation_time': 'first',
    'source_name': 'first',
    'destination_name': 'last',
    'od_start_time': 'first',
    'od_end_time': 'last',
    'start_scan_to_end_scan': 'sum',
    'actual_distance_to_destination': 'sum',
    'actual_time': 'sum',
    'osrm_time': 'sum',
    'osrm_distance': 'sum',
    'segment_actual_time': 'sum',
    'segment_osrm_time': 'sum',
    'segment_osrm_distance': 'sum'
}

# Group data by trip_uuid to retain all important values
df_final = df_grouped.groupby(['trip_uuid']).agg(agg_funcs).reset_index()

"""**Extracting substrings fom features to prepare the data for actual analysis.**"""

def extract_location(location):
    # Handle None or NaN values

    if not isinstance(location, str) or pd.isna(location):
        return pd.Series(["Unknown", "Unknown", "Unknown"])
    # Extract state (inside parentheses)

    state = location.split("(")[-1].strip(")")
    location = location.split(" (")[0]  # Removing state from main part
    parts = location.split("_", 1)  # Splitting into city and place code

    city = parts[0] if len(parts) > 0 else "Unknown"
    place_code = parts[1] if len(parts) > 1 else "Unknown"

    return pd.Series([city, place_code, state])


df_final[['source_city','source_place_code','source_state']] = df_final['source_name'].apply(extract_location)
df_final[['destination_city','destination_place_code','destination_state']]= df_final['destination_name'].apply(extract_location)

df_final['trip_creation_time'] = pd.to_datetime(df_final['trip_creation_time'], errors='coerce')

# Extract year, month, day, hour, minute, weekday
df_final['trip_year'] = df_final['trip_creation_time'].dt.year
df_final['trip_month'] = df_final['trip_creation_time'].dt.month
df_final['trip_day'] = df_final['trip_creation_time'].dt.day

"""**In-depth analysis and feature engineering:**




"""

df_final['od_start_time'] = pd.to_datetime(df_final['od_start_time'],errors = 'coerce')
df_final['od_end_time'] = pd.to_datetime(df_final['od_end_time'], errors = 'coerce')

df_final['od_duration_hours'] = (df_final['od_end_time']-df_final['od_start_time']).dt.total_seconds()/3600

"""### **Performing hypothesis testing and visual analysis between actual_time aggregated value and OSRM time aggregated values.**

**Hypothesis Testing - Paired T-Test**
* Null Hypothesis (H₀): There is no significant difference between actual_time and osrm_time.
* Alternative Hypothesis (H₁): There is a significant difference between the two.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Compute difference
df_final['time_difference'] = df_final['actual_time'] - df_final['osrm_time']

# Plot histogram
plt.figure(figsize=(10,5))
sns.histplot(df_final['time_difference'], bins=50, kde=True, color="blue", alpha=0.6)

# Add vertical line at zero for reference
plt.axvline(x=0, color='red', linestyle='dashed', linewidth=2, label="No Difference")

# Labels
plt.title("Distribution of Difference: Actual Time vs. OSRM Time")
plt.xlabel("Difference (Actual Time - OSRM Time)")
plt.ylabel("Frequency")
plt.legend()
plt.show()

# Perform paired t-test
t_stat, p_value = ttest_rel(df_final['actual_time'], df_final['osrm_time'])

print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_value:.4f}")

# Interpretation
if p_value < 0.05:
    print("Reject Null Hypothesis: There is a significant difference between Actual Time and OSRM Time.")
else:
    print("Fail to Reject Null Hypothesis: No significant difference found.")

"""**Insight:**
* There is no statistically significant difference between Actual Time and OSRM Time at the 95% confidence level.

### **Performing hypothesis testing/ visual analysis between actual_time aggregated value and segment actual time aggregated value**

**Hypothesis Testing**
* Null Hypothesis (H₀): There is no significant difference between aggregated actual time and aggregated segment actual time.
* Alternative Hypothesis (H₁): There is a significant difference between the two, indicating inconsistencies in trip breakdowns.
"""

plt.figure(figsize=(10,5))
df_final['time_diff'] = df_final['segment_actual_time'] - df_final['actual_time']
sns.histplot(df_final['time_diff'], bins=50, kde=True, color="blue", alpha=0.7)
plt.axvline(0, color='red', linestyle='dashed', label="No Difference")
plt.title("Distribution of Difference: Segment Actual Time vs. Actual Time")
plt.xlabel("Difference (Segment Actual Time - Actual Time)")
plt.ylabel("Frequency")
plt.legend()
plt.show()

# Perform a paired t-test
t_stat, p_value = stats.ttest_rel(df_final['segment_actual_time'], df_final['actual_time'])

# Print hypothesis test results
print("T-Statistic:", t_stat)
print("P-Value:", p_value)

# Decision Based on P-Value
alpha = 0.05  # Significance level
if p_value < alpha:
    print("Reject the Null Hypothesis: Significant difference exists.")
else:
    print("Fail to Reject Null Hypothesis: No significant difference.")

"""**Insights and Recommendations:**
* There are inconsistencies between segment-wise tracking and total trip duration.

* Overall, segment-based actual times tend to be overestimated compared to actual trip durations.

* This could mean that individual segment travel times do not accurately reflect real-world trip conditions, leading to inflated total durations when summed.
* Investigate specific routes or regions where overestimation is extreme and adjust segment time assumptions accordingly.

### **Performing Hypothesis Testing and Visual analysis between osrm distance aggregated value and segment osrm distance aggregated value**

* Null Hypothesis (H₀): The aggregated osrm_distance and segment_osrm_distance are similar (no significant difference).

* Alternative Hypothesis (H₁): The aggregated osrm_distance and segment_osrm_distance are significantly different.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Compute difference
df_final["distance_diff"] = df_final["segment_osrm_distance"] - df_final["osrm_distance"]

# Plot distribution
plt.figure(figsize=(10, 5))
sns.histplot(df_final["distance_diff"], bins=50, kde=True, color="blue", alpha=0.7)
plt.axvline(0, color="red", linestyle="dashed", label="No Difference")
plt.xlabel("Difference (Segment OSRM Distance - OSRM Distance)")
plt.ylabel("Frequency")
plt.title("Distribution of Difference: Segment OSRM Distance vs. OSRM Distance")
plt.legend()
plt.show()

from scipy import stats
import numpy as np

# Compute mean of the difference
mean_diff = np.mean(df_final["distance_diff"])

# Perform one-sample t-test
t_stat, p_value = stats.ttest_1samp(df_final["distance_diff"], 0)

# Print results
print(f"Mean Difference: {mean_diff:.4f}")
print(f"T-Statistic: {t_stat:.4f}")
print(f"P-Value: {p_value:.4f}")

# Confidence level
alpha = 0.05

# Check for overestimation or underestimation
if p_value < alpha:
    if mean_diff < 0:
        print("OSRM distances are significantly OVERestimated.")
    else:
        print("OSRM distances are significantly UNDERestimated.")
else:
    print("No significant difference found. OSRM distance predictions are accurate.")

"""### **Hypothesis Testing & Visual Analysis: OSRM Time vs. Segment OSRM Time**

* Null Hypothesis (H₀): There is no significant difference between OSRM Time and Segment OSRM Time.
* Alternative Hypothesis (H₁): There is a significant difference, meaning OSRM's aggregated time and segment-wise time do not align.
"""

df_final['osrm_time_diff'] = df_final['segment_osrm_time'] - df_final['osrm_time']
plt.figure(figsize=(10, 5))
sns.histplot(df_final['osrm_time_diff'], bins=50, kde=True, color='blue')
plt.axvline(0, color='red', linestyle='--', label="No Difference")
plt.title("Distribution of Difference: Segment OSRM Time vs. OSRM Time")
plt.xlabel("Difference (Segment OSRM Time - OSRM Time)")
plt.ylabel("Frequency")
plt.legend()
plt.show()

t_stat, p_value = ttest_rel(df_final['segment_osrm_time'], df_final['osrm_time'])

print(f"T-Statistic: {t_stat:.3f}, P-Value: {p_value:.5f}")

# Check significance level
if p_value < 0.05:
    print("Reject Null Hypothesis : Significant difference found between OSRM Time and Segment OSRM Time.")

    if df_final['osrm_time_diff'].mean() > 0:
        print("OSRM is underestimating travel time.")
    else:
        print("OSRM is overestimating travel time.")
else:
    print("Fail to Reject Null Hypothesis : No significant difference, OSRM time is consistent.")

"""**Insights and Recommendations:**
* It is observed that OSRM time is being underestimated.
* Enhance OSRM by integrating real-time traffic, road conditions and historical delays to improve time estimations.

**Outlier Handling:**
"""

import numpy as np
numerical_cols = df_final.select_dtypes(include=np.number).columns


# Function to detect outliers using IQR
def detect_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Identifying outliers
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers

# Create subplots
num_cols = len(numerical_cols)
rows = (num_cols // 3) + (num_cols % 3 > 0)  # Arrange in 3 columns per row

fig, axes = plt.subplots(rows, 3, figsize=(15, rows * 4))
axes = axes.flatten()  # Flatten the 2D array of axes

# Plot each numerical column
for i, col in enumerate(numerical_cols):
    sns.boxplot(x=df_final[col], ax=axes[i],color='pink')
    axes[i].set_title(f'Boxplot of {col}')
    axes[i].set_xlabel("")
# Hide empty subplots (if any)
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.figure(figsize=(12, 6))
plt.tight_layout()
plt.show()

"""**Outlier Handling: Capping the outliers**"""

def cap_outliers_iqr(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Capping outliers
    df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])
    df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])

    return df

for col in numerical_cols:
    df_final = cap_outliers_iqr(df_final, col)

"""**One-hot encoding of categorical variables (like route_type):**"""

df_encoded = pd.get_dummies(df, columns=['route_type'], prefix='route')

"""**Standardizing the numerical features using StandardScaler:**"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

num_cols = ['actual_distance_to_destination', 'actual_time', 'osrm_time', 'osrm_distance', 'segment_osrm_distance']

df_final[num_cols] = scaler.fit_transform(df_final[num_cols])

"""### **Business Insights**"""

# Count the number of shipments per source state
top_source_states = df_final['source_state'].value_counts().head(10)

# Count the number of shipments per source-destination pair (corridor)
top_corridors = df_final.groupby(['source_state', 'destination_state']).size().reset_index(name='count')
top_corridors = top_corridors.sort_values(by='count', ascending=False).head(10)

print("Top Source States:", top_source_states)
print("Top Corridors:", top_corridors)

"""**Insights:**
* Maharashtra has the highest number of shipments (2,714), followed by Karnataka (2,143) and Haryana (1,838).


* Most shipments happen within the same state. The busiest shipping corridors are within Maharashtra (2,453 orders), Karnataka (2,057 orders), and Tamil Nadu (1,021 orders). This indicates a high demand for intra-state deliveries, likely due to local e-commerce and distribution networks.

**Recommendations:**

* Optimize intra-state logistics: Since most orders take place within the same state, Delhivery should increase warehouse capacity and last-mile delivery speed in high-demand states. Investing in regional warehouses help in reducing transit costs and delivery times.

### **Key Recommendations:**

* **Improve OSRM Accuracy:** Fine-tune routing models with real-time traffic data to reduce distance/time estimation errors.

* **Optimize High-Traffic Corridors:** Improve fleet allocation and explore alternate routes for busy corridors like Maharashtra -> Maharashtra.

* Optimize loading/unloading processes to minimize delays.
"""